# ğŸ§  Chatbot documentaire avec LLM & RAG

Ce projet montre comment utiliser un **LLM** et une approche **RAG (Retrieval-Augmented Generation)** pour interroger intelligemment un corpus de documents (PDF, textes mÃ©tier).

## ğŸ¯ Objectif

Permettre Ã  un utilisateur mÃ©tier de poser des questions en langage naturel sur une base documentaire et dâ€™obtenir des rÃ©ponses :
- prÃ©cises,
- sourcÃ©es,
- contextualisÃ©es par les documents dâ€™origine.

## ğŸ§± Architecture

1. **Ingestion** des documents (`data/raw/`)
2. **Vectorisation** (embeddings) et crÃ©ation dâ€™un index (`data/processed/`)
3. **RAG** : rÃ©cupÃ©ration des passages pertinents
4. **GÃ©nÃ©ration de la rÃ©ponse** par le LLM Ã  partir du contexte

_Un schÃ©ma de lâ€™architecture est disponible dans `assets/schema.png`._

## ğŸ› ï¸ Stack technique

- Python
- Langage de modÃ¨le : LLM type GPT / open-source (selon dispo)
- BibliothÃ¨ques :
  - `langchain` ou Ã©quivalent
  - `faiss` / `chromadb` / autre vecteur store
  - `pandas`, `numpy`
  - `streamlit` (optionnel si interface web)

## ğŸ“ Structure du projet

Voir lâ€™arborescence dÃ©taillÃ©e dans le repo.

## ğŸš€ Lancer le projet

```bash
# Cloner le repo
git clone https://github.com/Gwaldyso/01-llm-rag-chatbot.git
cd 01-llm-rag-chatbot

# CrÃ©er et activer un environnement virtuel (optionnel mais recommandÃ©)

# Installer les dÃ©pendances
pip install -r requirements.txt

# Lancer le script principal (exemple)
python src/app.py

