# üß† Chatbot documentaire avec LLM & RAG

Ce projet montre comment utiliser un **LLM** et une approche **RAG (Retrieval-Augmented Generation)** pour interroger intelligemment un corpus de documents (PDF, textes m√©tier).

##  Objectif

Permettre √† un utilisateur m√©tier de poser des questions en langage naturel sur une base documentaire et d‚Äôobtenir des r√©ponses :
- pr√©cises,
- sourc√©es,
- contextualis√©es par les documents d‚Äôorigine.

##  Architecture

1. **Ingestion** des documents (`data/raw/`)
2. **Vectorisation** (embeddings) et cr√©ation d‚Äôun index (`data/processed/`)
3. **RAG** : r√©cup√©ration des passages pertinents
4. **G√©n√©ration de la r√©ponse** par le LLM √† partir du contexte

_Un sch√©ma de l‚Äôarchitecture est disponible dans `assets/schema.png`._

##  Stack technique

- Python
- Langage de mod√®le : LLM type GPT / open-source (selon dispo)
- Biblioth√®ques :
  - `langchain` ou √©quivalent
  - `faiss` / `chromadb` / autre vecteur store
  - `pandas`, `numpy`
  - `streamlit` (optionnel si interface web)

##  Structure du projet

Voir l‚Äôarborescence d√©taill√©e dans le repo.

##  Lancer le projet


# Cloner le repo
git clone https://github.com/Gwaldyso/01-llm-rag-chatbot.git
cd 01-llm-rag-chatbot

# Cr√©er et activer un environnement virtuel (optionnel mais recommand√©)

# ü§ñ Chatbot Documentaire RAG ‚Äî Retrieval-Augmented Generation

Ce projet impl√©mente un **chatbot documentaire intelligent**, capable de r√©pondre √† des questions en langage naturel en utilisant vos propres documents (PDF, textes, rapports m√©tier‚Ä¶).

Il repose sur une architecture **RAG (Retrieval-Augmented Generation)** combinant :

- embeddings HuggingFace,
- une base vectorielle ChromaDB,
- et un mod√®le de g√©n√©ration OpenAI (GPT-4.1-mini par d√©faut).

Ce type de pipeline est aujourd‚Äôhui utilis√© en entreprise pour :  
- automatiser du support,  
- analyser des documents internes,  
- interroger des bases documentaires m√©tier,  
- cr√©er des assistants LLM priv√©s.

---

#  Objectif

Permettre √† un utilisateur de poser des questions naturelles sur ses documents et d‚Äôobtenir des r√©ponses :

- pr√©cises  
- contextualis√©es  
- sourc√©es par des extraits r√©els  

---

#  Architecture du projet

Voici le pipeline complet :


#  Stack technique

### **Langages & Frameworks**
- Python 3.10+
- Streamlit (application web)

### **LLM & NLP**
- HuggingFace SentenceTransformers ‚Üí `all-MiniLM-L6-v2` pour les embeddings  
- OpenAI GPT-4.1-mini (ou tout autre mod√®le compatible) pour la g√©n√©ration

### **Vector Database**
- ChromaDB (persistant)

### **Autres biblioth√®ques**
- `pandas`, `numpy`
- `openai`
- `chromadb`
- `sentence-transformers`



#  Structure du projet


01-llm-rag-chatbot/
 ‚îú‚îÄ‚îÄ data/
 ‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Fichiers d‚Äôentr√©e (PDF, TXT‚Ä¶)
 ‚îÇ   ‚îî‚îÄ‚îÄ processed/
 ‚îÇ        ‚îî‚îÄ‚îÄ chroma_db/       # Base vectorielle persistante
 ‚îú‚îÄ‚îÄ src/
 ‚îÇ   ‚îú‚îÄ‚îÄ ingest.py             # Extraction texte + cr√©ation des chunks
 ‚îÇ   ‚îú‚îÄ‚îÄ build_index.py        # Embeddings + insertion dans Chroma
 ‚îÇ   ‚îú‚îÄ‚îÄ app.py                # Version console du chatbot RAG
 ‚îÇ   ‚îî‚îÄ‚îÄ app_streamlit.py      # Application Streamlit
 ‚îú‚îÄ‚îÄ requirements.txt
 ‚îú‚îÄ‚îÄ .gitignore
 ‚îî‚îÄ‚îÄ README.md                 # (ce fichier)


##  Lancer le projet
bash
# Cloner le repo
- git clone https://github.com/Gwaldyso/01-llm-rag-chatbot.git
- cd 01-llm-rag-chatbot

# Cr√©er et activer un environnement virtuel (optionnel mais recommand√©)

- python -m venv .venv
- .\.venv\Scripts\activate    # Windows


>>>>>>> 3409a5748b4ccdf7bafb835a8a1594003e8564af
# Installer les d√©pendances
pip install -r requirements.txt

# Lancer le script principal (exemple)
python src/app.py



## Pipeline d‚Äôutilisation
# √âtape 1 ‚Äî D√©poser vos documents

Placez vos fichiers PDF/TXT dans :

data/raw/

---

# √âtape 2 ‚Äî Ingestion & chunking
- Ex√©cutez le script d‚Äôingestion pour extraire le texte et cr√©er les chunks :
- Le script g√©n√®re automatiquement :

---

# √âtape 3 ‚Äî Construction de l‚Äôindex vectoriel
- Construisez l‚Äôindex (embeddings + stockage ChromaDB) :
- L‚Äôindex vectoriel persistant sera cr√©√© dans :

---

# √âtape 4 ‚Äî Version console (CLI)
- Configurez votre cl√© OpenAI : 
- Lancez le chatbot en mode console :

---

# √âtape 5 ‚Äî Version web (Streamlit)
- Exportez votre cl√© OpenAI :
- Lancez l‚Äôinterface web :


- Acc√©dez ensuite √† :  
   http://localhost:8501

- Vous verrez :
  - une zone pour poser vos questions  
  - une r√©ponse g√©n√©r√©e par le LLM  
  - un mode debug pour voir les chunks utilis√©s  


## ‚öôÔ∏è Fonctionnalit√©s

###  Chatbot documentaire intelligent
- Posez des questions en langage naturel sur vos documents.
- Obtenez des r√©ponses contextualis√©es et structur√©es.

###  Pipeline RAG complet
- Ingestion et d√©coupage des documents en chunks.
- Vectorisation (embeddings) et indexation dans une base vectorielle.
- Recherche s√©mantique des passages les plus pertinents.
- G√©n√©ration de la r√©ponse en s‚Äôappuyant sur le contexte.

###  Deux modes d‚Äôutilisation
- Mode console (CLI) : interaction dans le terminal.
- Interface web Streamlit : chatbot accessible via navigateur.

###  Architecture modulaire
- Scripts s√©par√©s pour :
  - l‚Äôingestion (`ingest.py`)
  - la construction de l‚Äôindex (`build_index.py`)
  - le chatbot console (`app.py`)
  - le chatbot web (`app_streamlit.py`)


##  Fonctionnement d√©taill√©

###  Embeddings ‚Äî HuggingFace MiniLM-L6-v2
- Mod√®le : `sentence-transformers/all-MiniLM-L6-v2`
- Caract√©ristiques :
  - rapide
  - l√©ger
  - excellent rapport qualit√© / vitesse
- Utilisation :
  - transforme les questions et les chunks de texte en vecteurs num√©riques.
  - permet de mesurer la similarit√© entre la question et les passages de documents.

---

###  Vectorisation & Recherche ‚Äî ChromaDB
- Base vectorielle : **ChromaDB** (mode persistant).
- R√¥le :
  - stocker les embeddings des chunks.
  - faire de la recherche de similarit√© (kNN) pour retrouver les passages les plus proches de la question.
- Avantages :
  - simple √† utiliser en Python.
  - adapt√©e aux projets RAG de petite et moyenne taille.

---

###  G√©n√©ration ‚Äî OpenAI GPT-4.1-mini
- Mod√®le utilis√© : `gpt-4.1-mini` (configurable).
- R√¥le :
  - recevoir un **prompt** contenant :
    - le contexte (chunks retrouv√©s)
    - la question utilisateur
    - des consignes de r√©ponse (en fran√ßais, strictement bas√©e sur le contexte).
  - g√©n√©rer une r√©ponse claire, synth√©tique et structur√©e.
- Avantages :
  - bonne qualit√© de langage.
  - adapt√© √† des t√¢ches de r√©sum√©, r√©ponse √† des questions, reformulation.

---

##  Points forts du projet

###  Architecture RAG compl√®te
- Couverture de toutes les √©tapes :
  - ingestion
  - embeddings
  - indexation
  - retrieval
  - g√©n√©ration

###  Combinaison de technologies modernes
- Embeddings HuggingFace + ChromaDB + OpenAI :
  - montre une bonne compr√©hension des outils actuels de l‚ÄôIA g√©n√©rative.
  - facilement transposable dans un contexte entreprise.

###  Code structur√© et p√©dagogique
- S√©paration claire des responsabilit√©s :
  - `ingest.py` pour la pr√©paration des donn√©es.
  - `build_index.py` pour la construction de l‚Äôindex.
  - `app.py` pour la version console.
  - `app_streamlit.py` pour la version web.
- Facile √† lire, √† maintenir et √† √©tendre.







